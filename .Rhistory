data$AgeuponOutcome <- gsub(" days?","",data$AgeuponOutcome)
data$AgeuponOutcome <- as.numeric(paste0("0",data$AgeuponOutcome))
#MIX:
data$mix <- 1*(grepl("Mix", data$Breed, fixed = TRUE))
#BREED:
# seems to matter mostly for cats & PitBull:
gsub(" Mix", "", data$Breed) -> temp
strsplit(x = temp, split = "/") %>% sapply(function(x){x[1]}) -> data$breed1
count(data, breed1) %>%
arrange(desc(n)) %>%
filter(n >75) -> popular
data$breed1[!(data$breed1 %in% popular$breed1)] <- "Exotic"
#COLORS:
strsplit(x = data$Color, split = "/") %>% sapply(function(x){x[1]}) -> data$color1
data %>% count(color1) %>% arrange(desc(n)) %>% filter(n > 200) -> colors
data$color1[!(data$color1 %in% colors$color1)] <- "othercolor"
#dash in the breed:
data$dash <- grepl("/", data$Breed)
#number of letter in the name:
data$namelength <- nchar(data$Name)
#see how many levels you have
sapply(data, function(x){unique(x) %>% length()})
train <- cbind(data[1:dim(train)[1],], OutcomeType = y)
test <- cbind(data[-(1:dim(train)[1]),])
#let's also create the design_matrices.
design_matrix <- sparse.model.matrix( ~   DateTime +
AnimalType +
SexuponOutcome +
age +
AgeuponOutcome +
weekend +
hour +
breed1 +
namelength +
named +
wday +
mix,
data = data)[,-1]
design_matrix_train <- design_matrix[1:dim(train)[1],]
design_matrix_test <- design_matrix[-(1:dim(train)[1]),]
new_y <- as.numeric(y) - 1 # for xgboost.
save.image("data.Rdata")
library(glmnet) #for regularized logistic regression
library(ranger) #for random forests
library(xgboost) #for boosted trees
library(dplyr)
library(LiblineaR)
library(ggplot2)
load("data.Rdata")
model_xgb <- xgboost(data = design_matrix_train,
label = new_y,
nround = 250,
eta = 0.1,
objective = "multi:softprob",
eval_metric = "mlogloss",
num_class = 5,
max.depth = 6,
min_child_weight = 1)
predict(model_xgb, design_matrix_test) -> temp
solution <- t(matrix(temp, nrow = 5))
colnames(solution) <- levels(y)
solution <- data.frame(ID = test$ID, solution)
View(solution)
write.csv(solution, "xgboosttry_withnumericfeats2.csv", row.names = FALSE)
#this actually works!!!! - so keep as template for predicting probabilities with xgboost.
solution$Adoption[1:10]
full <- data
full$TimeValue <- sapply(full$AgeuponOutcome,
function(x) strsplit(x, split = ' ')[[1]][1])
full$AgeuponOutcome
train <- read.csv("data/train.csv", stringsAsFactors = FALSE)
test <- read.csv("data/test.csv", stringsAsFactors = FALSE)
y <- as.factor(train$OutcomeType)
names(train)[1] <- "ID"
data <- rbind(train[,-c(4,5)], test)
full <- data
# Get the time value:
full$TimeValue <- sapply(full$AgeuponOutcome,
function(x) strsplit(x, split = ' ')[[1]][1])
# Now get the unit of time:
full$UnitofTime <- sapply(full$AgeuponOutcome,
function(x) strsplit(x, split = ' ')[[1]][2])
# Fortunately any "s" marks the plural, so we can just pull them all out
full$UnitofTime <- gsub('s', '', full$UnitofTime)
full$TimeValue  <- as.numeric(full$TimeValue)
full$UnitofTime <- as.factor(full$UnitofTime)
View(full)
multiplier <- ifelse(full$UnitofTime == 'day', 1,
ifelse(full$UnitofTime == 'week', 7,
ifelse(full$UnitofTime == 'month', 30, # Close enough
ifelse(full$UnitofTime == 'year', 365, NA))))
# Apply our multiplier
full$AgeinDays <- full$TimeValue * multiplier
data$AgeinDays <- full$AgeinDays
data$named <- 1*(data$Name != "")
#let's start with DATETIME:
data %>%
mutate(year = year(data$DateTime),
month = month(data$DateTime),
day = day(data$DateTime),
hour = hour(data$DateTime),
wday = wday(data$DateTime)) -> data
data$DateTime <- as.numeric(as.POSIXct(data$DateTime))
#make a weekend var:
data$weekend <- 1*data$wday %in% c(1, 7)
# so I should use weekend, year, month and hour - no NA's
#SEXUPONOUTCOME:
# seems important:
data$SexuponOutcome[data$SexuponOutcome == ""] <- "Unknown"
#AGEUPONOUTCOME:
data$age  <- data$AgeuponOutcome
data$age[grepl("day", data$AgeuponOutcome)] <- "< a week"
data$age[grepl("week", data$AgeuponOutcome)] <-  "< a month"
data$age[data$age == ""] <- "1 year"
data$age[data$age %in% c("13 years", "14 years", "15 years", "16 years", "17 years", "18 years",
"19 years", "20 years")] <- "> 12 years"
data$AgeuponOutcome <- gsub(" years?","0000",data$AgeuponOutcome)
data$AgeuponOutcome <- gsub(" months?","00",data$AgeuponOutcome)
data$AgeuponOutcome <- gsub(" weeks?","0",data$AgeuponOutcome)
data$AgeuponOutcome <- gsub(" days?","",data$AgeuponOutcome)
data$AgeuponOutcome <- as.numeric(paste0("0",data$AgeuponOutcome))
#MIX:
data$mix <- 1*(grepl("Mix", data$Breed, fixed = TRUE))
#BREED:
# seems to matter mostly for cats & PitBull:
gsub(" Mix", "", data$Breed) -> temp
strsplit(x = temp, split = "/") %>% sapply(function(x){x[1]}) -> data$breed1
count(data, breed1) %>%
arrange(desc(n)) %>%
filter(n >75) -> popular
data$breed1[!(data$breed1 %in% popular$breed1)] <- "Exotic"
#COLORS:
strsplit(x = data$Color, split = "/") %>% sapply(function(x){x[1]}) -> data$color1
data %>% count(color1) %>% arrange(desc(n)) %>% filter(n > 200) -> colors
data$color1[!(data$color1 %in% colors$color1)] <- "othercolor"
#dash in the breed:
data$dash <- grepl("/", data$Breed)
#number of letter in the name:
data$namelength <- nchar(data$Name)
#see how many levels you have
sapply(data, function(x){unique(x) %>% length()})
train <- cbind(data[1:dim(train)[1],], OutcomeType = y)
test <- cbind(data[-(1:dim(train)[1]),])
# now we're good.
#let's also create the design_matrices.
design_matrix <- sparse.model.matrix( ~   DateTime +
AnimalType +
SexuponOutcome +
age +
AgeuponOutcome +
weekend +
hour +
breed1 +
namelength +
named +
wday +
mix,
data = data)[,-1]
design_matrix_train <- design_matrix[1:dim(train)[1],]
design_matrix_test <- design_matrix[-(1:dim(train)[1]),]
new_y <- as.numeric(y) - 1 # for xgboost.
save.image("data.Rdata")
design_matrix <- sparse.model.matrix( ~   DateTime +
AnimalType +
SexuponOutcome +
age +
AgeinDays +
weekend +
hour +
breed1 +
namelength +
named +
wday +
mix,
data = data)[,-1]
design_matrix <- sparse.model.matrix( ~   DateTime +
AnimalType +
SexuponOutcome +
age +
AgeinDays +
weekend +
hour +
breed1 +
namelength +
named +
wday +
mix,
data = data)[,-1]
View(as.matrix(design_matrix))
data$DateTime <- scale(data$DateTime)
#let's also create the design_matrices.
design_matrix <- sparse.model.matrix( ~   DateTime +
AnimalType +
SexuponOutcome +
age +
AgeinDays +
weekend +
hour +
breed1 +
namelength +
named +
wday +
mix,
data = data)[,-1]
design_matrix_train <- design_matrix[1:dim(train)[1],]
design_matrix_test <- design_matrix[-(1:dim(train)[1]),]
new_y <- as.numeric(y) - 1 # for xgboost.
View(as.matrix(design_matrix))
nround = 300
cv_xgb <- xgb.cv(data = design_matrix_train,label = new_y,
nround = nround,
eta = 0.1,
objective = "multi:softprob",
eval_metric = "mlogloss",
num_class = 5,
max.depth = 6,
nfold = 5,
min_child_weight = 1)
View(as.matrix(design_matrix_train))
str(design_matrix)
cv_xgb <- xgb.cv(data = design_matrix_train[,-1],label = new_y,
nround = nround,
eta = 0.1,
objective = "multi:softprob",
eval_metric = "mlogloss",
num_class = 5,
max.depth = 6,
nfold = 5,
min_child_weight = 1)
library(dplyr)
library(ggplot2)
library(lubridate)
library(tidyr)
library(Matrix)
train <- read.csv("data/train.csv", stringsAsFactors = FALSE)
test <- read.csv("data/test.csv", stringsAsFactors = FALSE)
y <- as.factor(train$OutcomeType)
names(train)[1] <- "ID"
data <- rbind(train[,-c(4,5)], test)
#NAME:
data$named <- 1*(data$Name != "")
#let's start with DATETIME:
data %>%
mutate(year = year(data$DateTime),
month = month(data$DateTime),
day = day(data$DateTime),
hour = hour(data$DateTime),
wday = wday(data$DateTime)) -> data
data$DateTime <- as.numeric(as.POSIXct(data$DateTime))
#make a weekend var:
data$weekend <- 1*data$wday %in% c(1, 7)
# so I should use weekend, year, month and hour - no NA's
#SEXUPONOUTCOME:
# seems important:
data$SexuponOutcome[data$SexuponOutcome == ""] <- "Unknown"
#AGEUPONOUTCOME:
data$age  <- data$AgeuponOutcome
data$age[grepl("day", data$AgeuponOutcome)] <- "< a week"
data$age[grepl("week", data$AgeuponOutcome)] <-  "< a month"
data$age[data$age == ""] <- "1 year"
data$age[data$age %in% c("13 years", "14 years", "15 years", "16 years", "17 years", "18 years",
"19 years", "20 years")] <- "> 12 years"
data$AgeuponOutcome <- gsub(" years?","0000",data$AgeuponOutcome)
data$AgeuponOutcome <- gsub(" months?","00",data$AgeuponOutcome)
data$AgeuponOutcome <- gsub(" weeks?","0",data$AgeuponOutcome)
data$AgeuponOutcome <- gsub(" days?","",data$AgeuponOutcome)
data$AgeuponOutcome <- as.numeric(paste0("0",data$AgeuponOutcome))
#MIX:
data$mix <- 1*(grepl("Mix", data$Breed, fixed = TRUE))
#BREED:
# seems to matter mostly for cats & PitBull:
gsub(" Mix", "", data$Breed) -> temp
strsplit(x = temp, split = "/") %>% sapply(function(x){x[1]}) -> data$breed1
count(data, breed1) %>%
arrange(desc(n)) %>%
filter(n >75) -> popular
data$breed1[!(data$breed1 %in% popular$breed1)] <- "Exotic"
#COLORS:
strsplit(x = data$Color, split = "/") %>% sapply(function(x){x[1]}) -> data$color1
data %>% count(color1) %>% arrange(desc(n)) %>% filter(n > 200) -> colors
data$color1[!(data$color1 %in% colors$color1)] <- "othercolor"
#dash in the breed:
data$dash <- grepl("/", data$Breed)
#number of letter in the name:
data$namelength <- nchar(data$Name)
#
data$DateTime <- scale(data$DateTime)
#see how many levels you have
sapply(data, function(x){unique(x) %>% length()})
train <- cbind(data[1:dim(train)[1],], OutcomeType = y)
test <- cbind(data[-(1:dim(train)[1]),])
# now we're good.
#let's also create the design_matrices.
design_matrix <- sparse.model.matrix( ~   DateTime +
AnimalType +
SexuponOutcome +
age +
AgeinDays +
weekend +
hour +
breed1 +
namelength +
named +
wday +
mix,
data = data)[,-1]
design_matrix_train <- design_matrix[1:dim(train)[1],]
design_matrix_test <- design_matrix[-(1:dim(train)[1]),]
new_y <- as.numeric(y) - 1 # for xgboost.
save.image("data.Rdata")
nround = 300
cv_xgb <- xgb.cv(data = design_matrix_train,label = new_y,
nround = nround,
eta = 0.1,
objective = "multi:softprob",
eval_metric = "mlogloss",
num_class = 5,
max.depth = 6,
nfold = 5,
min_child_weight = 1)
design_matrix_train <- design_matrix[1:dim(train)[1],]
#let's also create the design_matrices.
design_matrix <- sparse.model.matrix( ~   DateTime +
AnimalType +
SexuponOutcome +
age +
AgeinDays +
weekend +
hour +
breed1 +
namelength +
named +
wday +
mix,
data = data)[,-1]
#let's also create the design_matrices.
design_matrix <- sparse.model.matrix( ~   DateTime +
AnimalType +
SexuponOutcome +
age +
AgeuponOutcome +
weekend +
hour +
breed1 +
namelength +
named +
wday +
mix,
data = data)[,-1]
design_matrix_train <- design_matrix[1:dim(train)[1],]
design_matrix_test <- design_matrix[-(1:dim(train)[1]),]
new_y <- as.numeric(y) - 1 # for xgboost.
cv_xgb <- xgb.cv(data = design_matrix_train,label = new_y,
nround = nround,
eta = 0.1,
objective = "multi:softprob",
eval_metric = "mlogloss",
num_class = 5,
max.depth = 6,
nfold = 5,
min_child_weight = 1)
model_rf <- ranger(OutcomeType ~DateTime +
AnimalType +
SexuponOutcome +
age +
AgeuponOutcome +
weekend +
hour +
breed1 +
namelength +
named +
wday +
mix, , # breed1,
data = train, mtry = 4,
num.trees = 200, #probability = TRUE,
importance = "impurity", write.forest = TRUE,
seed = 3231L)
model_rf
model_rf <- ranger(OutcomeType ~DateTime +
AnimalType +
SexuponOutcome +
age +
AgeuponOutcome +
weekend +
hour +
breed1 +
namelength +
named +
wday +
mix, # breed1,
data = train, mtry = 4,
num.trees = 200, probability = TRUE,
importance = "impurity", write.forest = TRUE,
seed = 3231L)
model_rf
#variable importance:
model_rf$variable.importance %>% sort(decreasing = TRUE) %>%
barplot(las = 1, main = "Variable Importance for Random Forest")
#variable importance:
model_rf$variable.importance %>% sort(decreasing = TRUE)
predict(model_rf, test) ->rf_pred
cv_xgb <- xgb.cv(data = design_matrix_train,label = new_y,
nround = nround,
eta = 0.1,
objective = "multi:softprob",
#eval_metric = "mlogloss",
num_class = 5,
max.depth = 6,
nfold = 5,
min_child_weight = 1)
predict(model_rf, test) ->rf_pred
rf_pred <- rf_pred$predictions
View(rf_pred)
predict(model_xgb, design_matrix_test) -> temp
solution <- t(matrix(temp, nrow = 5))
model_xgb <- xgboost(data = design_matrix_train,
label = new_y,
nround = 250,
eta = 0.1,
objective = "multi:softprob",
eval_metric = "mlogloss",
num_class = 5,
max.depth = 6,
min_child_weight = 1)
predict(model_xgb, design_matrix_test) -> temp
solution <- t(matrix(temp, nrow = 5))
colnames(solution) <- levels(y)
solution_stack <- (rf_pred + solution)/2
solution_stack$ID <- test$ID
solution_stack <- (rf_pred + solution)/2
View(solution_stack)
solution_stack <- data.frame((rf_pred + solution)/2)
solution_stack$ID <- test$ID
write.csv(solution_stack, "xgbplusrf.csv", row.names = FALSE)
solution_rf <- data.frame(rf_pred)
solution_stack$ID <- test$ID
View(solution_rf)
solution_rf$ID <- test$ID
write.csv(solution_rf, "rf_numeric_breed1.csv", row.names = FALSE)
dir()
best <- read.csv("xgboosttry_withnumericfeats.csv")
best_rf <- (2*best + solution_rf)/3
best_rf <- (best + solution_rf)/2
View(best)
best_rf <- (best[,-1] + solution_rf[,-1])/2
best_rf <- (best + solution_rf[,-1])/2
best_rf <- (best[-1,] + solution_rf)/2
best_rf <- (best[,-1] + solution_rf)/2
best_rf <- (best[,-1] + rf_pred)/2
best_rf$ID <- test$ID
write.csv(best_rf, "rf+bestxgb.csv", row.names = FALSE)
model_rf <- ranger(OutcomeType ~DateTime +
AnimalType +
SexuponOutcome +
age +
AgeuponOutcome +
weekend +
hour +
breed1 +
namelength +
named +
wday +
mix,
data = train, mtry = 4,
num.trees = 800, probability = TRUE,
importance = "impurity", write.forest = TRUE,
seed = 3231L)
#performs much better if we use probability trees. + we probabilities are better
#for kaggle since the metric is multi logloss.
model_rf$prediction.error #27.6% OOB error for the probability tree - seems weird.
#variable importance:
model_rf$variable.importance %>% sort(decreasing = TRUE) %>%
barplot(las = 1, main = "Variable Importance for Random Forest")
predict(model_rf, test) ->rf_pred
rf_pred <- rf_pred$predictions
cv_xgb <- xgb.cv(data = design_matrix_train,label = new_y,
nround = nround,
eta = 0.1,
objective = "multi:softprob",
#eval_metric = "mlogloss",
num_class = 5,
max.depth = 6,
nfold = 5,
min_child_weight = 1)
cv_xgb <- xgb.cv(data = design_matrix_train,label = new_y,
nround = nround,
eta = 0.1,
objective = "multi:softprob",
eval_metric = "mlogloss",
num_class = 5,
max.depth = 6,
nfold = 5,
min_child_weight = 5)
cv_xgb <- xgb.cv(data = design_matrix_train,label = new_y,
nround = nround,
eta = 0.1,
objective = "multi:softprob",
eval_metric = "mlogloss",
num_class = 5,
max.depth = 6,
nfold = 5,
min_child_weight = 1,
gamma = 0.1)
best_rf <- (2*best[,-1] + rf_pred)/3
best_rf$ID <- test$ID
View(best_rf)
write.csv(best_rf, "rf800trees+bestxgb.csv", row.names = FALSE)
model_rf
best <- read.csv("xgboosttry_withnumericfeats.csv")
best_rf <- (best[,-1] + rf_pred)/2
best_rf$ID <- test$ID
write.csv(best_rf, "rf800trees+bestxgb.csv", row.names = FALSE)
