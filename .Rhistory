data = data)[,-1]
design_matrix_train <- design_matrix[1:dim(train)[1],]
new_y <- as.numeric(y) - 1 # for xgboost.
#something is broken with this design_matrix,
#probably the new features.
design_matrix <- sparse.model.matrix( ~   DateTime +
AnimalType +
SexuponOutcome +
age +
less_month +
AgeuponOutcome +
weekend +
hourz +
minute0 + #new
minute +
time + #new
miniature + #new
agressive + #new
breed1 +
namelength +
named +
wday +
mix,
data = data)[,-1]
design_matrix_train <- design_matrix[1:dim(train)[1],]
design_matrix_test <- design_matrix[-(1:dim(train)[1]),]
nround = 200
cv_xgb <- xgb.cv(data = design_matrix_train,label = new_y,
nround = nround,
eta = 0.1,
objective = "multi:softprob",
eval_metric = "mlogloss",
#eval_metric = "merror",
num_class = 5,
max.depth = 6,
nfold = 5,
min_child_weight = 1,
subsample=0.75,
colsample_bytree=0.85,
gamma = 0.1)
model_xgb <- xgboost(data = design_matrix_train,
label = new_y,
nround = 250,
eta = 0.1,
objective = "multi:softprob",
eval_metric = "mlogloss",
num_class = 5,
max.depth = 6,
min_child_weight = 1)
#let's look at importance
xgb.importance(feature_names = colnames(design_matrix_train), model = model_xgb) -> importance
ggplot(importance, aes( x = reorder(Feature, Gain), y = Gain)) +
geom_bar(stat = "identity") +
coord_flip()
importance
dim(design_matrix)
hist(train$AgeuponOutcome)
design_matrix <- sparse.model.matrix( ~   DateTime +
AnimalType +
SexuponOutcome +
age +
less_month +
log(AgeuponOutcome + 1) +
weekend +
hourz +
minute0 + #new
minute +
time + #new
miniature + #new
agressive + #new
breed1 +
namelength +
named +
wday +
mix,
data = data)[,-1]
hist(log(train$AgeuponOutcome +1))
design_matrix_train <- design_matrix[1:dim(train)[1],]
design_matrix_test <- design_matrix[-(1:dim(train)[1]),]
nround = 200
cv_xgb <- xgb.cv(data = design_matrix_train,label = new_y,
nround = nround,
eta = 0.1,
objective = "multi:softprob",
eval_metric = "mlogloss",
#eval_metric = "merror",
num_class = 5,
max.depth = 6,
nfold = 5,
min_child_weight = 1,
subsample=0.75,
colsample_bytree=0.85,
gamma = 0.1)
train <- read.csv("data/train.csv", stringsAsFactors = FALSE)
test <- read.csv("data/test.csv", stringsAsFactors = FALSE)
y <- as.factor(train$OutcomeType)
train <- read.csv("data/train.csv", stringsAsFactors = FALSE)
test <- read.csv("data/test.csv", stringsAsFactors = FALSE)
y <- as.factor(train$OutcomeType)
View(train)
data$DateTime <- as.numeric(as.POSIXct(data$DateTime))
names(train)[1] <- "ID" #different format vs test
data <- rbind(train[,-c(4,5)], test)
data$DateTime <- as.numeric(as.POSIXct(data$DateTime))
hist(train$DateTime)
hist(data$DateTime)
data$DateTime[1]
data$DateTime[1]/(60*60)
data$DateTime[1]/(60*60*24)
data$DateTime[1]/(60*60*24*300)
data <- rbind(train[,-c(4,5)], test)
data$DateTime[1]
range(data$year(DateTime))
range(year(data$DateTime))
data %>% mutate(timez = (year(DateTime) - 2013))
data %>% stransmute(timez = (year(DateTime) - 2013))
data %>% transmute(timez = (year(DateTime) - 2013))
library(dplyr)
library(ggplot2)
library(lubridate)
library(tidyr)
library(Matrix)
train <- read.csv("data/train.csv", stringsAsFactors = FALSE)
test <- read.csv("data/test.csv", stringsAsFactors = FALSE)
y <- as.factor(train$OutcomeType)
names(train)[1] <- "ID" #different format vs test
data <- rbind(train[,-c(4,5)], test)
#NAME:
data$named <- 1*(data$Name != "")
#number of letter in the name:
data$namelength <- nchar(data$Name)
#let's start with DATETIME:
data %>%
mutate(year = year(DateTime),
month = month(DateTime),
day = day(DateTime),
hour = hour(DateTime),
wday = wday(DateTime),
hourz =  hour(DateTime) + minute(DateTime)/60) -> data
data$minute <- minute(data$DateTime)
data$minute0 <- 1*(data$minute == 0)
data$DateTime <- as.numeric(as.POSIXct(data$DateTime))
data$time <- data$hour*60 + data$minute
#make a weekend var:
data$weekend <- 1*data$wday %in% c(1, 7)
# so I should use weekend, year, month and hour - no NA's
#SEXUPONOUTCOME:
# seems important:
data$SexuponOutcome[data$SexuponOutcome == ""] <- "Unknown"
strsplit(data$SexuponOutcome, " ") %>% sapply(function(x){x[1]}) -> data$neuter
strsplit(data$SexuponOutcome, " ") %>% sapply(function(x){x[2]}) -> data$sex
#AGEUPONOUTCOME:
data$less_month <- grepl("day|week", data$AgeuponOutcome)
data$age  <- data$AgeuponOutcome
data$age[grepl("day", data$AgeuponOutcome)] <- "< a week"
data$age[grepl("week", data$AgeuponOutcome)] <-  "< a month"
data$age[data$age == ""] <- "1 year"
data$age[data$age %in% c("13 years", "14 years", "15 years", "16 years", "17 years", "18 years",
"19 years", "20 years")] <- "> 12 years"
data$AgeuponOutcome <- gsub(" years?","0000",data$AgeuponOutcome)
data$AgeuponOutcome <- gsub(" months?","00",data$AgeuponOutcome)
data$AgeuponOutcome <- gsub(" weeks?","0",data$AgeuponOutcome)
data$AgeuponOutcome <- gsub(" days?","",data$AgeuponOutcome)
data$AgeuponOutcome <- as.numeric(paste0("0",data$AgeuponOutcome))
#MIX:
data$mix <- 1*(grepl("Mix", data$Breed, fixed = TRUE))
#BREED:
# seems to matter mostly for cats & PitBull:
gsub(" Mix", "", data$Breed) -> temp
data$miniature <- grepl("Miniature", data$Breed)
data$agressive <- grepl("Pit Bull|Rottweiler|Siberian Huskey|Boxer", data$Breed)
strsplit(x = temp, split = "/") %>% sapply(function(x){x[1]}) -> data$breed1
strsplit(x = temp, split = "/") %>% sapply(function(x){x[2]}) -> data$breed2
count(data, breed1) %>% arrange(desc(n)) %>% filter(n >75) -> popular
data$breed1[!(data$breed1 %in% popular$breed1)] <- "Exotic"
#COLORS:
strsplit(x = data$Color, split = "/") %>% sapply(function(x){x[1]}) -> data$color1
data %>% count(color1) %>% arrange(desc(n)) %>% filter(n > 200) -> colors
data$color1[!(data$color1 %in% colors$color1)] <- "othercolor"
#dash in the breed:
data$dash <- grepl("/", data$Breed)
#scale Date-Time:
data$DateTime <- scale(data$DateTime)
#see how many levels you have
sapply(data, function(x){unique(x) %>% length()})
train <- cbind(data[1:dim(train)[1],], OutcomeType = y)
test <- cbind(data[-(1:dim(train)[1]),])
# now we're good.
new_y <- as.numeric(y) - 1 # for xgboost.
design_matrix <- sparse.model.matrix( ~   DateTime +
AnimalType +
SexuponOutcome +
age +
AgeuponOutcome +
weekend +
hour +
minute +
breed1 +
namelength +
named +
wday +
mix,
data = data)[,-1]
design_matrix <- sparse.model.matrix( ~   DateTime +
AnimalType +
SexuponOutcome +
age +
less_month +
AgeuponOutcome +
weekend +
hourz +
minute0 + #new
minute +
time + #new
miniature + #new
agressive + #new
breed1 +
namelength +
named +
wday +
mix,
data = data)[,-1]
design_matrix_train <- design_matrix[1:dim(train)[1],]
design_matrix_test <- design_matrix[-(1:dim(train)[1]),]
dim(design_matrix_test)
dim(design_matrix_train)
model_xgb <- xgboost(data = design_matrix_train,
label = new_y,
nround = 300,
eta = 0.1,
objective = "multi:softprob",
eval_metric = "mlogloss",
num_class = 5,
max.depth = 6,
min_child_weight = 1,
subsample=0.75,
colsample_bytree=0.85,
gamma = 0.1)
predict(model_xgb, design_matrix_test) -> temp
solution <- t(matrix(temp, nrow = 5))
colnames(solution) <- levels(y)
solution <- data.frame(ID = test$ID, solution)
View(solution)
write.csv(solution, "xgboosttry_withnumericft+minutes+hourz.csv", row.names = FALSE)
#keep as template for predicting probabilities with xgboost.
nround = 400
cv_xgb <- xgb.cv(data = design_matrix_train,label = new_y,
nround = nround,
eta = 0.1,
objective = "multi:softprob",
eval_metric = "mlogloss",
#eval_metric = "merror",
num_class = 5,
max.depth = 6,
nfold = 5,
min_child_weight = 1,
subsample=0.75,
colsample_bytree=0.85,
gamma = 0.1)
nround = 200
cv_xgb <- xgb.cv(data = design_matrix_train,label = new_y,
nround = nround,
eta = 0.1,
objective = "multi:softprob",
eval_metric = "mlogloss",
#eval_metric = "merror",
num_class = 5,
max.depth = 8,
nfold = 5,
min_child_weight = 1,
subsample=0.75,
colsample_bytree=0.85,
gamma = 0.1)
nround = 200
cv_xgb <- xgb.cv(data = design_matrix_train,label = new_y,
nround = nround,
eta = 0.1,
objective = "multi:softprob",
eval_metric = "mlogloss",
#eval_metric = "merror",
num_class = 5,
max.depth = 10,
nfold = 5,
min_child_weight = 1,
subsample=0.75,
colsample_bytree=0.85,
gamma = 0.1)
nround = 400
cv_xgb <- xgb.cv(data = design_matrix_train,label = new_y,
nround = nround,
eta = 0.05,
objective = "multi:softprob",
eval_metric = "mlogloss",
#eval_metric = "merror",
num_class = 5,
max.depth = 8,
nfold = 5,
min_child_weight = 1,
subsample=0.75,
colsample_bytree=0.85,
gamma = 0.1)
model_xgb <- xgboost(data = design_matrix_train,
label = new_y,
nround = 350,
eta = 0.05,
objective = "multi:softprob",
eval_metric = "mlogloss",
num_class = 5,
max.depth = 8,
min_child_weight = 1,
subsample=0.75,
colsample_bytree=0.85,
gamma = 0.1)
predict(model_xgb, design_matrix_test) -> temp
solution <- t(matrix(temp, nrow = 5))
colnames(solution) <- levels(y)
solution <- data.frame(ID = test$ID, solution)
write.csv(solution, "xgboosttry_withnumericft+minutes+hourz2.csv", row.names = FALSE)
#keep as template for predicting probabilities with xgboost.
View(solution)
replicate(15, {
model_xgb <- xgboost(data = design_matrix_train,
label = new_y,
nround = 350,
eta = 0.05,
objective = "multi:softprob",
eval_metric = "mlogloss",
num_class = 5,
max.depth = 8,
min_child_weight = 1,
subsample=0.75,
colsample_bytree=0.85,
gamma = 0.1);
predict(model_xgb, design_matrix_test)}) -> predz
View(data)
data  %>% count(DateTime)
library(dplyr)
library(ggplot2)
library(lubridate)
library(tidyr)
library(Matrix)
train <- read.csv("data/train.csv", stringsAsFactors = FALSE)
test <- read.csv("data/test.csv", stringsAsFactors = FALSE)
y <- as.factor(train$OutcomeType)
names(train)[1] <- "ID" #different format vs test
data <- rbind(train[,-c(4,5)], test)
library(dplyr)
library(ggplot2)
library(lubridate)
library(tidyr)
library(Matrix)
train <- read.csv("data/train.csv", stringsAsFactors = FALSE)
test <- read.csv("data/test.csv", stringsAsFactors = FALSE)
y <- as.factor(train$OutcomeType)
names(train)[1] <- "ID" #different format vs test
data <- rbind(train[,-c(4,5)], test)
data %>% count(DateTime)
train %>% count(DateTime)
train %>% count(DateTime) %>% arrange(desc(n))
train %>% count(DateTime) %>% arrange(desc(n)) -> temp
View(temp)
data[data$DateTime == "2015-08-11 00:00:00"]
data[data$DateTime == "2015-08-11 00:00:00",]
train[train$DateTime == "2015-08-11 00:00:00",]
train[train$DateTime == "2015-11-17 00:00:00",]
right_join(temp, data, by = "DateTime") ->newz
View(newz)
data %>% count(DateTime) %>% arrange(desc(n))
data %>% count(DateTime) %>% arrange(desc(n)) -> temp
data %>% count(DateTime) %>% arrange(desc(n)) -> temp
right_join(temp, data, by = "DateTime") -> newz
train <- read.csv("data/train.csv", stringsAsFactors = FALSE)
test <- read.csv("data/test.csv", stringsAsFactors = FALSE)
y <- as.factor(train$OutcomeType)
names(train)[1] <- "ID" #different format vs test
data <- rbind(train[,-c(4,5)], test)
data %>% count(DateTime) %>% arrange(desc(n)) -> temp
right_join(temp, data, by = "DateTime") -> data
View(data)
#NAME:
data$named <- 1*(data$Name != "")
#number of letter in the name:
data$namelength <- nchar(data$Name)
#let's start with DATETIME:
data %>%
mutate(year = year(DateTime),
month = month(DateTime),
day = day(DateTime),
hour = hour(DateTime),
wday = wday(DateTime),
hourz =  hour(DateTime) + minute(DateTime)/60) -> data
data$minute <- minute(data$DateTime)
data$minute0 <- 1*(data$minute == 0)
data$DateTime <- as.numeric(as.POSIXct(data$DateTime))
data$time <- data$hour*60 + data$minute
#make a weekend var:
data$weekend <- 1*data$wday %in% c(1, 7)
# so I should use weekend, year, month and hour - no NA's
#SEXUPONOUTCOME:
# seems important:
data$SexuponOutcome[data$SexuponOutcome == ""] <- "Unknown"
strsplit(data$SexuponOutcome, " ") %>% sapply(function(x){x[1]}) -> data$neuter
strsplit(data$SexuponOutcome, " ") %>% sapply(function(x){x[2]}) -> data$sex
#AGEUPONOUTCOME:
data$less_month <- grepl("day|week", data$AgeuponOutcome)
data$age  <- data$AgeuponOutcome
data$age[grepl("day", data$AgeuponOutcome)] <- "< a week"
data$age[grepl("week", data$AgeuponOutcome)] <-  "< a month"
data$age[data$age == ""] <- "1 year"
data$age[data$age %in% c("13 years", "14 years", "15 years", "16 years", "17 years", "18 years",
"19 years", "20 years")] <- "> 12 years"
data$AgeuponOutcome <- gsub(" years?","0000",data$AgeuponOutcome)
data$AgeuponOutcome <- gsub(" months?","00",data$AgeuponOutcome)
data$AgeuponOutcome <- gsub(" weeks?","0",data$AgeuponOutcome)
data$AgeuponOutcome <- gsub(" days?","",data$AgeuponOutcome)
data$AgeuponOutcome <- as.numeric(paste0("0",data$AgeuponOutcome))
#MIX:
data$mix <- 1*(grepl("Mix", data$Breed, fixed = TRUE))
#BREED:
# seems to matter mostly for cats & PitBull:
gsub(" Mix", "", data$Breed) -> temp
data$miniature <- grepl("Miniature", data$Breed)
data$agressive <- grepl("Pit Bull|Rottweiler|Siberian Huskey|Boxer", data$Breed)
strsplit(x = temp, split = "/") %>% sapply(function(x){x[1]}) -> data$breed1
strsplit(x = temp, split = "/") %>% sapply(function(x){x[2]}) -> data$breed2
count(data, breed1) %>% arrange(desc(n)) %>% filter(n >75) -> popular
data$breed1[!(data$breed1 %in% popular$breed1)] <- "Exotic"
#COLORS:
strsplit(x = data$Color, split = "/") %>% sapply(function(x){x[1]}) -> data$color1
data %>% count(color1) %>% arrange(desc(n)) %>% filter(n > 200) -> colors
data$color1[!(data$color1 %in% colors$color1)] <- "othercolor"
#dash in the breed:
data$dash <- grepl("/", data$Breed)
#scale Date-Time:
data$DateTime <- scale(data$DateTime)
#see how many levels you have
sapply(data, function(x){unique(x) %>% length()})
train <- cbind(data[1:dim(train)[1],], OutcomeType = y)
test <- cbind(data[-(1:dim(train)[1]),])
# now we're good.
design_matrix <- sparse.model.matrix( ~   DateTime +
AnimalType +
SexuponOutcome +
age +
less_month +
AgeuponOutcome +
weekend +
hourz +
minute0 + #new
minute +
n + #LEAK
time + #new
miniature + #new
agressive + #new
breed1 +
namelength +
named +
wday +
mix,
data = data)[,-1]
design_matrix_train <- design_matrix[1:dim(train)[1],]
design_matrix_test <- design_matrix[-(1:dim(train)[1]),]
nround = 400
cv_xgb <- xgb.cv(data = design_matrix_train,label = new_y,
nround = nround,
eta = 0.05,
objective = "multi:softprob",
eval_metric = "mlogloss",
#eval_metric = "merror",
num_class = 5,
max.depth = 8,
nfold = 5,
min_child_weight = 1,
subsample=0.75,
colsample_bytree=0.85,
gamma = 0.1)
new_y <- as.numeric(y) - 1 # for xgboost.
nround = 400
cv_xgb <- xgb.cv(data = design_matrix_train,label = new_y,
nround = nround,
eta = 0.05,
objective = "multi:softprob",
eval_metric = "mlogloss",
#eval_metric = "merror",
num_class = 5,
max.depth = 8,
nfold = 5,
min_child_weight = 1,
subsample=0.75,
colsample_bytree=0.85,
gamma = 0.1)
model_xgb <- xgboost(data = design_matrix_train,
label = new_y,
nround = 350,
eta = 0.05,
objective = "multi:softprob",
eval_metric = "mlogloss",
num_class = 5,
max.depth = 8,
min_child_weight = 1,
subsample=0.75,
colsample_bytree=0.85,
gamma = 0.1)#;
predict(model_xgb, design_matrix_test) -> temp
solution <- t(matrix(temp, nrow = 5))
colnames(solution) <- levels(y)
solution <- data.frame(ID = test$ID, solution)
write.csv(solution, "xgboosttry_withnumericft+minutes+hourz2+leak.csv", row.names = FALSE)
#keep as template for predicting probabilities with xgboost.
2+2
2+2
2+52345234512
